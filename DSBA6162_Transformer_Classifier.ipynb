{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv2YR+YmgxIchXXsOxHU51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/owilli38/DSBA-6162/blob/main/DSBA6162_Transformer_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c7CqgfceRCm"
      },
      "outputs": [],
      "source": [
        "#!pip install tf-keras\n",
        "#!pip install transformers==4.42.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tf_keras\n",
        "print(tf_keras.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2NVQhXYfBw2",
        "outputId": "2b839516-29d5-4422-ce44-eca11a167db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "\n",
        "vocab_size = 20000\n",
        "max_length = 200\n",
        "embed_dim = 32\n",
        "ff_dim = 32\n",
        "num_heads = 2\n",
        "num_labels = 3"
      ],
      "metadata": {
        "id": "-lCt9RZUfEXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86c08652"
      },
      "source": [
        "class TokenAndPositionEmbedding(tf_keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, vocabulary_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.token_embedding = tf_keras.layers.Embedding(\n",
        "            input_dim=vocabulary_size, output_dim=embedding_dim\n",
        "        )\n",
        "        self.position_embedding = tf_keras.layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        position_indices = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embedding(inputs)\n",
        "        embedded_positions = self.position_embedding(position_indices)\n",
        "        return embedded_tokens + embedded_positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76db9f15"
      },
      "source": [
        "class TransformerBlock(tf_keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = tf_keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf_keras.Sequential(\n",
        "            [tf_keras.layers.Dense(ff_dim, activation=\"relu\"), tf_keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf_keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf_keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf_keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf_keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "input_layer = tf_keras.layers.Input(shape=(max_length,))\n",
        "embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n",
        "x = embedding_layer(input_layer)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = tf_keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = tf_keras.layers.Dropout(0.1)(x)\n",
        "x = tf_keras.layers.Dense(20, activation=\"relu\")(x)\n",
        "output_layer = tf_keras.layers.Dense(num_labels, activation=\"softmax\")(x)\n",
        "\n",
        "classifier_model = tf_keras.Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "502f5e80",
        "outputId": "d1e3a8fc-9db8-46eb-8f1d-ee319b4747ee"
      },
      "source": [
        "vocab_size = 20000\n",
        "max_length = 200\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf_keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "x_train = tf_keras.utils.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = tf_keras.utils.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "print(len(x_train), \"Training sequences after padding\")\n",
        "print(len(x_test), \"Testing sequences after padding\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "25000 Training sequences after padding\n",
            "25000 Testing sequences after padding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIyWxX3GlW2L",
        "outputId": "f8b43278-0717-4227-b478-fb3bb8b0dbdf"
      },
      "source": [
        "epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "classifier_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = classifier_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 127s 156ms/step - loss: 0.3942 - accuracy: 0.8043 - val_loss: 0.2838 - val_accuracy: 0.8803\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 138s 176ms/step - loss: 0.1925 - accuracy: 0.9270 - val_loss: 0.3080 - val_accuracy: 0.8746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a467593c",
        "outputId": "e5a87b3c-5f5d-4430-d7bf-9864eb0a4b85"
      },
      "source": [
        "vocab_size = 20000\n",
        "max_length = 200\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf_keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "x_train = tf_keras.utils.pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = tf_keras.utils.pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "print(len(x_train), \"Training sequences after padding\")\n",
        "print(len(x_test), \"Testing sequences after padding\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences after padding\n",
            "25000 Testing sequences after padding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "980d51c1",
        "outputId": "d72208fa-e1dd-4ea1-ce1f-fc6854eb2ff0"
      },
      "source": [
        "print(\"Training History:\")\n",
        "for epoch in range(len(history.history['loss'])):\n",
        "    print(f\"Epoch {epoch + 1}:\")\n",
        "    print(f\"  Training Loss: {history.history['loss'][epoch]:.4f}\")\n",
        "    print(f\"  Training Accuracy: {history.history['accuracy'][epoch]:.4f}\")\n",
        "    if 'val_loss' in history.history:\n",
        "        print(f\"  Validation Loss: {history.history['val_loss'][epoch]:.4f}\")\n",
        "        print(f\"  Validation Accuracy: {history.history['val_accuracy'][epoch]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training History:\n",
            "Epoch 1:\n",
            "  Training Loss: 0.3942\n",
            "  Training Accuracy: 0.8043\n",
            "  Validation Loss: 0.2838\n",
            "  Validation Accuracy: 0.8803\n",
            "Epoch 2:\n",
            "  Training Loss: 0.1925\n",
            "  Training Accuracy: 0.9270\n",
            "  Validation Loss: 0.3080\n",
            "  Validation Accuracy: 0.8746\n"
          ]
        }
      ]
    }
  ]
}